#### XGBoost学习方法

v1.0 2090713

- 先思考一下自己是不是天才，如果是请忽略下面的学习步骤
- 对于初学者不要看到各种Kaggle比赛、腾讯广告算法大神的大佬用了LGBM（XGBoost变种）等XGBoost算法就想着经典机器学习算法不想看了先学习最牛逼的算法的思想
- 所有牛逼的算法都有一个牛逼的爹。XGBoost的很牛逼是因为有多个牛逼的爹。列举部分如下：决策树爹、正则爹、泰勒公式爹、随机森林爹（列采样基因）、排序爹（Boosting为何能够做到并行）、梯度下降爹……
- 你应该先去看李航《统计学习方法》中关于CART决策树的基础理论，掌握一颗决策树是如何遍历所有切分点然后找到最佳切分点的
- 你应该先去学习梯度下降算法，至少应该看懂一元函数和多元函数求偏导数的方法、如此你还需要去复习一下极限理论
- 你应该先去学习决策树的损失函数，了解决策树的损失函数是如何控制分裂点
- 你应该先去学习正则化的思想，强烈建议去看网易云课堂的吴恩达机器学习关于正则化的相关解释，非常适合我这种白痴，至少应该懂得，惩罚项到底在干嘛，我一开始看到惩罚项，以为是前面的损失函数做错了什么事情，例如私下河塘洗澡之类的需要受到惩罚，损失函数到底做错了什么。正则化L1和L2两种，至少应该理解经典的等高线图的交叉点概念
- 先学习Boosting算法。掌握加法模型和Bagging算法的不同之处在哪儿，比如和随机森林的区别点在什么地方
- 然后去掌握提升树算法，理解提升树在拟合数据的时候使用残差的概念，实际上就是在求定义为平方损失函数的梯度。
- 然后去掌握梯度提升树算法，理解为何需要升级成这个算法，在面临什么场景的时候，提升树算法就不灵了，需要使用到梯度来求解类似SoftMax这样的损失函数
- 然后你理解了上面的GBDT，你可以开始尝试去学习XGBoost，别急，看到蛋疼的原论文的时候很痛苦，找一下知乎，找一下github的可能是东半球最大的学习组织（罗永浩？）等地方，慢慢的啃，一步步的来，才是适合我这样的白痴学习方法。
